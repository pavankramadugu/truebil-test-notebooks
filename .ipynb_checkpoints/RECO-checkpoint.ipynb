{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Set, OrderedDict\n",
    "class OrderedSet(Set):\n",
    "    def __init__(self, iterable=()):\n",
    "        self.d = OrderedDict.fromkeys(iterable)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.d)\n",
    "\n",
    "    def __contains__(self, element):\n",
    "        return element in self.d\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.d)\n",
    "    \n",
    "TRENDING_POPULAR_MODELS_QUERY = \"\"\"SELECT v.model_id, COUNT(v.model_id) AS num\n",
    "                                    FROM variants v\n",
    "                                        JOIN listings l ON v.id = l.variant_id\n",
    "                                        JOIN models m ON m.id = v.model_id\n",
    "                                    WHERE l.is_inventory = 1\n",
    "                                        AND v.model_id IN {trending_models}\n",
    "                                        {is_trending}\n",
    "                                        AND l.id IN {popular_listings}\n",
    "                                    GROUP BY v.model_id\n",
    "                                    ORDER BY num DESC\"\"\"\n",
    "\n",
    "TRENDING_MODELS = [217, 471, 209, 468, 182, 76, 242, 464, 132, 463, 306, 260, 438, 314, 379, 202, 248, 449, 301, 363,\n",
    "                   365, 249, 70, 513]\n",
    "\n",
    "SUPER_SAVER_LISTINGS_QUERY = \"\"\"SELECT DISTINCT city_id, listing_id FROM supersaver_listings WHERE DATE(created_at) = (SELECT MAX(DATE(created_at)) FROM supersaver_listings)\"\"\"\n",
    "\n",
    "# Function to drop duplicates and fill NAN data\n",
    "def drop_fill(df):\n",
    "    return df.reset_index(drop=True).fillna(0).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import pymysql\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import redis\n",
    "import sentry_sdk\n",
    "import urlparse\n",
    "from ConfigParser import ConfigParser\n",
    "from lightfm import LightFM\n",
    "from scipy.sparse import coo_matrix, identity, hstack\n",
    "from sentry_sdk.integrations.redis import RedisIntegration\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "date_today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DB connection\n",
    "connection = pymysql.connect(passwd=\"newreadpassword\", db=\"truebil\", host=\"db.truebil.com\", user=\"truebil-read\")\n",
    "\n",
    "# Buyer filters\n",
    "buyer_filters = pd.read_sql_query(\n",
    "    \"\"\"SELECT DISTINCT buyer_id, filter FROM buyer_filters WHERE filter != ' ' ORDER BY created_at desc\"\"\",\n",
    "    connection).set_index('buyer_id', drop=True)\n",
    "\n",
    "unique_buyers = np.unique(buyer_filters.index)\n",
    "user_features_columns = ['buyer_id', 'price_b_2', 'price_bw_2_3', 'price_bw_3_4', 'price_bw_4_5', 'price_bw_5_8',\n",
    "                         'price_a_8', 'Hatchback', 'Sedan', 'MPV', 'SUV', 'Petrol', 'Diesel']\n",
    "price_slabs = user_features_columns[1:7]\n",
    "user_features = pd.DataFrame(columns=user_features_columns, dtype='int32')\n",
    "\n",
    "# Features data preparation\n",
    "fuel_types = {1: \"Petrol\", 2: \"Diesel\"}\n",
    "body_types = {1: \"Hatchback\", 2: \"Sedan\", 3: \"MPV\", 4: \"SUV\"}\n",
    "price_slabs_df = pd.DataFrame()\n",
    "price_slabs_df['min'] = [0, 200000, 300000, 400000, 500000, 800000]\n",
    "price_slabs_df['max'] = [200000, 300000, 400000, 500000, 800000, float(\"inf\")]\n",
    "\n",
    "# Function to generate user features\n",
    "all_users = []\n",
    "for buyer_id in unique_buyers:\n",
    "    feature_dict = {key: 0 for key in user_features_columns}\n",
    "    feature_dict[\"buyer_id\"] = buyer_id\n",
    "    buyer_filter = buyer_filters.loc[buyer_id, 'filter']\n",
    "    if type(buyer_filter) is str:\n",
    "        buyer_filter = [buyer_filter]\n",
    "\n",
    "    # For a unique buyer, get all filters and take feature wise union\n",
    "    column_names = {}\n",
    "    feature_data = {'price_max': [], 'price_min': [], 'fuel': [], 'body_type': []}\n",
    "    for filter_string in buyer_filter:\n",
    "        parsed_filter = dict(urlparse.parse_qs(filter_string))\n",
    "        for feature in feature_data.keys():\n",
    "            if feature in parsed_filter:\n",
    "                split_data = map(int, map(float, parsed_filter[feature][0].split(',')))\n",
    "                feature_data[feature].extend(split_data)\n",
    "            else:\n",
    "                feature_data[feature].extend([0])\n",
    "        column_names.update(feature_data)\n",
    "\n",
    "    fuel_type_ids = set(column_names['fuel'])\n",
    "    body_type_ids = set(column_names['body_type'])\n",
    "    price_max = max(column_names['price_max'])\n",
    "\n",
    "    for fuel_type in fuel_type_ids:\n",
    "        if fuel_type in fuel_types.keys():\n",
    "            feature_dict[fuel_types[fuel_type]] = 1\n",
    "\n",
    "    for body_type in body_type_ids:\n",
    "        if body_type in body_types.keys():\n",
    "            feature_dict[body_types[body_type]] = 1\n",
    "\n",
    "    # Set all price slabs between min and max price range\n",
    "    price_values = {key: 0 for key in price_slabs}\n",
    "    price_slabs_df['price_check'] = 0\n",
    "    if price_max != 0:\n",
    "        price_min = min(column_names['price_min'])  # price_min is always available when price_max is available\n",
    "        max_price_slab_index = price_slabs_df[(price_slabs_df['min'] < price_max) &\n",
    "                                              (price_slabs_df['max'] >= price_max)].index[0]\n",
    "        min_price_slab_index = price_slabs_df[(price_slabs_df['min'] <= price_min) &\n",
    "                                              (price_slabs_df['max'] > price_min)].index[0]\n",
    "        range_indices = list(range(min_price_slab_index, int(max_price_slab_index) + 1))\n",
    "        price_slabs_df.loc[range_indices, 'price_check'] = 1\n",
    "        price_values = price_slabs_df['price_check'].tolist()\n",
    "        price_values = dict(zip(price_slabs, price_values))\n",
    "        feature_dict.update(price_values)\n",
    "\n",
    "    user_features = user_features.append(feature_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "# Function to drop duplicates and fill NAN data\n",
    "def drop_fill(df):\n",
    "    return df.reset_index(drop=True).fillna(0).astype('int32')\n",
    "\n",
    "# Data Retrieval\n",
    "\n",
    "# Item features\n",
    "item_features = pd.read_sql_query(\n",
    "    \"(select listings.id as listing_id, localities.city_id, ifnull(price, 0)<200000, ifnull(price, 0) between 100000 and 300000 , ifnull(price, 0) between 200000 and 400000 , ifnull(price, 0) between 300000 and 500000, ifnull(price, 0) between 400000 and 600000,ifnull(price, 0) between 500000 and 700000 ,ifnull(price, 0) between 600000 and 800000, ifnull(price, 0) between 700000 and 900000, ifnull(price, 0)>900000, ifnull(listings.body_type_id=1,0), ifnull(listings.body_type_id=2, 0),ifnull(listings.body_type_id=3, 0), ifnull(listings.body_type_id=4, 0), listings.fuel_type_id_primary=1, listings.fuel_type_id_primary=2, ifnull(cast(listings.status as unsigned), 2) as status, ifnull(cast(listings.is_inventory as unsigned), 0) as is_inventory from listings join  localities on listings.locality_id = localities.id) union (select listings.id, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0 from listings where locality_id is null) \", connection)\n",
    "\n",
    "# Active listings and their features\n",
    "active_listings = item_features.query(\"status in (3, 9)\")[0:].reset_index()\n",
    "active_listings.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Max listing and Max user id\n",
    "max_listing_id = max(item_features['listing_id'])\n",
    "max_buyer_id = pd.read_sql_query(\"(select max(id) as id from buyers)\", connection)\n",
    "max_buyer_id = max_buyer_id['id'][0]\n",
    "\n",
    "# CRFs\n",
    "crfs = pd.read_sql_query(\n",
    "    \"(SELECT buyer_id, listing_id,date(buyer_listings.created_at) as created_at FROM buyer_listings JOIN listings ON buyer_listings.listing_id = listings.id JOIN \"\n",
    "    \"buyers ON buyer_listings.buyer_id = buyers.id)\",\n",
    "    connection).dropna()\n",
    "crfs = crfs[crfs['created_at'] != '0000-00-00']\n",
    "crfs['days'] = (date_today - crfs.loc[:,'created_at']).dt.days\n",
    "crfs = crfs.drop('created_at',1)\n",
    "\n",
    "# Offers\n",
    "offers = pd.read_sql_query(\n",
    "    \"(SELECT buyer_id, listing_id,date(buyer_listing_offers.created_at) as created_at FROM buyer_listing_offers JOIN listings ON \"\n",
    "    \"buyer_listing_offers.listing_id = listings.id JOIN \"\n",
    "    \"buyers ON buyer_listing_offers.buyer_id = buyers.id)\",\n",
    "    connection).dropna()\n",
    "offers = offers[offers['created_at'] != '0000-00-00']\n",
    "offers['days'] = (date_today - offers.loc[:,'created_at']).dt.days\n",
    "offers = offers.drop('created_at',1)\n",
    "\n",
    "# Is seen\n",
    "seen = pd.read_sql_query(\n",
    "    \"(SELECT buyers.id AS buyer_id, user_seens.listing_id, date(user_seens.created_at) as created_at FROM \"\n",
    "    \"user_seens JOIN users ON user_seens.user_id=users.id JOIN buyers ON users.mobile=buyers.mobile WHERE user_seens.seen_count>0)\",\n",
    "    connection).dropna()\n",
    "seen = seen[seen['created_at'] != '0000-00-00']\n",
    "seen['days'] = (date_today - seen.loc[:,'created_at']).dt.days\n",
    "seen = seen.drop('created_at', 1)\n",
    "\n",
    "# Is Shortlisted\n",
    "shortlisted = pd.read_sql_query(\n",
    "    \"(SELECT buyers.id AS buyer_id, user_shortlists.listing_id, date(user_shortlists.created_at) as created_at \"\n",
    "    \"FROM user_shortlists JOIN users ON user_shortlists.user_id=users.id JOIN buyers ON users.mobile=buyers.mobile WHERE user_shortlists.is_shortlisted=1)\",\n",
    "    connection).dropna()\n",
    "shortlisted = shortlisted[shortlisted['created_at'] != '0000-00-00']\n",
    "shortlisted['days'] = (date_today - shortlisted.loc[:,'created_at']).dt.days\n",
    "shortlisted = shortlisted.drop('created_at', 1)\n",
    "\n",
    "# Invalid Numbers\n",
    "invalid_numbers = pd.read_sql_query(\n",
    "    \"(SELECT buyers.id FROM buyers JOIN invalid_numbers ON buyers.mobile=invalid_numbers.mobile)\", connection).astype(\n",
    "    'int32')\n",
    "\n",
    "# Data CLeaning\n",
    "\n",
    "# Filling the missing listing ids info\n",
    "\n",
    "item_indices = np.array(range(1, max_listing_id + 1))\n",
    "item_ids = item_features['listing_id'].values\n",
    "\n",
    "item_features = item_features.append(pd.DataFrame(\n",
    "    ([i, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0] for i in np.setdiff1d(item_indices, item_ids)),\n",
    "    columns=item_features.columns), ignore_index=True)\n",
    "item_features = item_features.sort_values('listing_id')\n",
    "\n",
    "item_features = drop_fill(item_features)\n",
    "\n",
    "# Filling the missing buyer ids info\n",
    "\n",
    "user_indices = np.array(range(1, max_buyer_id + 1))\n",
    "user_ids = user_features['buyer_id'].values\n",
    "\n",
    "user_features = user_features.append(\n",
    "    pd.DataFrame(([i, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] for i in np.setdiff1d(user_indices, user_ids)),\n",
    "                 columns=user_features.columns), ignore_index=True)\n",
    "user_features = user_features.sort_values('buyer_id')\n",
    "\n",
    "user_features = drop_fill(user_features)\n",
    "\n",
    "# Item Features (needed for training) to csr matrix\n",
    "item_features_for_model = coo_matrix(item_features.values[:, 2:-2])\n",
    "item_features_csr = coo_matrix(identity(max_listing_id))\n",
    "\n",
    "item_features_final = coo_matrix(hstack([item_features_csr, item_features_for_model])).tocsr()\n",
    "\n",
    "# User Features (needed for training) to csr matrix\n",
    "user_features_for_model = coo_matrix(user_features.values[:, 1:])\n",
    "user_features_csr = coo_matrix(identity(max_buyer_id))\n",
    "\n",
    "user_features_final = coo_matrix(hstack([user_features_csr, user_features_for_model])).tocsr()\n",
    "\n",
    "seen['weight'] = 0.2-(seen['days']/365)*0.2\n",
    "shortlisted['weight'] =  0.5 - (shortlisted['days']/365)*0.5\n",
    "crfs['weight'] = 1 - (crfs['days']/365)*0.75\n",
    "offers['weight'] = 1 - (offers['days']/365)*0.75\n",
    "\n",
    "seen.loc[seen[seen['weight']<0].index.values,'weight'] = 0\n",
    "shortlisted.loc[shortlisted[shortlisted['weight']<0].index.values,'weight'] = 0\n",
    "crfs.loc[crfs[crfs['weight']<0].index.values,'weight'] = 0.25\n",
    "offers.loc[offers[offers['weight']<0].index.values,'weight'] = 0.25\n",
    "\n",
    "seen = seen.drop(seen[(seen.buyer_id>max_buyer_id) | (seen.buyer_id).isin(invalid_numbers['id'])].index)\n",
    "shortlisted = shortlisted.drop(shortlisted[(shortlisted.buyer_id>max_buyer_id) | (shortlisted.buyer_id).isin(invalid_numbers['id'])].index)\n",
    "crfs = crfs.drop(crfs[(crfs.buyer_id>max_buyer_id) | (crfs.buyer_id).isin(invalid_numbers['id'])].index)\n",
    "offers = offers.drop(offers[(offers.buyer_id>max_buyer_id) | (offers.buyer_id).isin(invalid_numbers['id'])].index)\n",
    "\n",
    "seen_array = seen.drop('days',1).drop_duplicates(subset=['buyer_id','listing_id']).values\n",
    "shortlisted_array = shortlisted.drop('days',1).drop_duplicates(subset=['buyer_id','listing_id']).values\n",
    "crfs_array = crfs.drop('days',1).drop_duplicates(subset=['buyer_id','listing_id']).values\n",
    "offers_array = offers.drop('days',1).drop_duplicates(subset=['buyer_id','listing_id']).values\n",
    "\n",
    "sample_weights_array = np.concatenate((seen_array, shortlisted_array, crfs_array, offers_array))\n",
    "sample_weights_array[:, 0:2] = sample_weights_array[:, 0:2] - 1\n",
    "interactions_array = copy.deepcopy(sample_weights_array)\n",
    "interactions_array[:, 2] = 1\n",
    "\n",
    "interactions_matrix = coo_matrix((interactions_array[:,2], (interactions_array[:,0], interactions_array[:,1])),\n",
    "                                 shape=(max_buyer_id, max_listing_id))\n",
    "sample_weights_matrix = coo_matrix((sample_weights_array[:,2], (sample_weights_array[:,0], sample_weights_array[:,1])),\n",
    "                                   shape=(max_buyer_id, max_listing_id))\n",
    "\n",
    "# Model training\n",
    "model = LightFM(loss='bpr')\n",
    "model = model.fit(interactions_matrix, item_features=item_features_final, user_features=user_features_final,\n",
    "                  sample_weight=sample_weights_matrix, epochs=30)\n",
    "pool = redis.ConnectionPool(host=\"127.0.0.1\", port=6379, db=1)\n",
    "r = redis.Redis(connection_pool=pool)\n",
    "r.set('model', pickle.dumps(model))\n",
    "r.set('user_features', pickle.dumps(user_features_final))\n",
    "r.set('item_features', pickle.dumps(item_features_final))\n",
    "r.set('max_buyer_id', max_buyer_id)\n",
    "r.set('training_interval', 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_shuffled_inventory(inventory_listings):\n",
    "    inventory_size = np.size(inventory_listings)\n",
    "    percent_5_inventory_size = int(0.05 * inventory_size)\n",
    "    shuffle_num = max(5, percent_5_inventory_size)\n",
    "    change_order_listings = inventory_listings[:shuffle_num]\n",
    "    random.seed(datetime.now().hour)\n",
    "    rotate = random.randint(1, shuffle_num)\n",
    "    change_order_listings = deque(change_order_listings)\n",
    "    change_order_listings.rotate(rotate)\n",
    "    inventory_listings[:shuffle_num] = change_order_listings\n",
    "    return inventory_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_super_saver_listings():\n",
    "    global SUPER_SAVER_DF\n",
    "    SUPER_SAVER_DF = pd.read_sql_query(SUPER_SAVER_LISTINGS_QUERY, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_super_saver_listings(listings, city_id):\n",
    "    listing_recommendation = list(listings)\n",
    "    city_super_saver = SUPER_SAVER_DF[SUPER_SAVER_DF['city_id'] == city_id]['listing_id'].tolist()\n",
    "    city_super_saver_set = OrderedSet(city_super_saver)\n",
    "    listings_set = OrderedSet(listings)\n",
    "    listings_set -= city_super_saver_set\n",
    "    listings = list(listings_set)\n",
    "\n",
    "    s_index = 0\n",
    "    super_saver_listings = list(city_super_saver_set)\n",
    "    ordered_super_saver = []\n",
    "    for listing in listing_recommendation:\n",
    "        if listing in super_saver_listings:\n",
    "            ordered_super_saver.append(listing)\n",
    "\n",
    "    final_listings = [ordered_super_saver[s_index]]\n",
    "    s_index += 1\n",
    "\n",
    "    for iterator in xrange(0, len(listings), 3):\n",
    "        final_listings.extend(listings[iterator:iterator + 3])\n",
    "        if s_index < len(ordered_super_saver):\n",
    "            final_listings.append(ordered_super_saver[s_index])\n",
    "            s_index += 1\n",
    "\n",
    "    final_listings.extend(ordered_super_saver[s_index:])\n",
    "    final_listings = np.array(final_listings)\n",
    "    return final_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_popular_listing_scores(sorted_popular_listings, item_features_bias_scores):\n",
    "    popular_listings = sorted_popular_listings.tolist()\n",
    "    item_features_scores = item_features_bias_scores.tolist()\n",
    "    scores = []\n",
    "    for listing in popular_listings:\n",
    "        scores.append(item_features_scores[listing])\n",
    "    scores = np.array(scores)\n",
    "    scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "    normalized_scores = scores.reshape(-1, 1)\n",
    "    scaler.fit(normalized_scores)\n",
    "    normalized_scores = scaler.transform(normalized_scores).ravel()\n",
    "    listings_scores_dictionary = dict(zip(sorted_popular_listings, normalized_scores))\n",
    "    return listings_scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_trending_popular_models(popular_listings, is_trending=False):\n",
    "    if is_trending is False:\n",
    "        condition = \" \"\n",
    "    else:\n",
    "        condition = \"\"\"AND l.price <= m.cutoff_price\"\"\"\n",
    "\n",
    "    query = TRENDING_POPULAR_MODELS_QUERY.format(is_trending=condition, trending_models=tuple(\n",
    "        map(str, TRENDING_MODELS)), popular_listings=tuple(map(str, popular_listings)))\n",
    "\n",
    "    models_df = pd.read_sql_query(query, connection)\n",
    "    inventory_wise_models = OrderedSet(models_df['model_id'].tolist())\n",
    "    all_trending_models = OrderedSet(TRENDING_MODELS)\n",
    "    no_inventory_models = list(all_trending_models - inventory_wise_models)\n",
    "    final_models = models_df['model_id'].tolist() + no_inventory_models\n",
    "\n",
    "    return final_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_recommendations():\n",
    "    item_features_bias_scores = model.item_biases\n",
    "    item_features_baises = model.item_biases.argsort()[::-1]\n",
    "    all_city_popular_listings = {}\n",
    "    get_super_saver_listings()\n",
    "\n",
    "    for city_id in np.unique(active_listings['city_id'].values):\n",
    "        city_active_listings = active_listings.query(\"city_id == %s\" % city_id)\n",
    "        popular_listings = item_features_baises[np.in1d(item_features_baises, city_active_listings['listing_id'].values)]\n",
    "        city_active_inventory_listings = city_active_listings[city_active_listings[\"is_inventory\"] == 1]['listing_id'].values\n",
    "        inventory_sorted_listings = popular_listings[np.in1d(popular_listings, city_active_inventory_listings)]\n",
    "        inventory_sorted_listings_unshuffled = deepcopy(inventory_sorted_listings)\n",
    "        inventory_sorted_listings = get_shuffled_inventory(inventory_sorted_listings)\n",
    "        marketplace_sorted_listings = popular_listings[~np.in1d(popular_listings, city_active_inventory_listings)]\n",
    "        sorted_popular_listings_unshuffled = np.append(inventory_sorted_listings_unshuffled,\n",
    "                                                       marketplace_sorted_listings)\n",
    "        sorted_popular_listings = np.append(inventory_sorted_listings, marketplace_sorted_listings)\n",
    "        sorted_popular_listings = set_super_saver_listings(sorted_popular_listings, city_id)\n",
    "        popular_listing_scores_dictionary = set_popular_listing_scores(sorted_popular_listings_unshuffled,\n",
    "                                                                       item_features_bias_scores)\n",
    "\n",
    "        trending_models = get_trending_popular_models(popular_listings=sorted_popular_listings_unshuffled,\n",
    "                                                      is_trending=True)\n",
    "        popular_models = get_trending_popular_models(popular_listings=sorted_popular_listings_unshuffled)\n",
    "\n",
    "        all_city_popular_listings[city_id] = {'popular_models': popular_models,\n",
    "                                              'trending_models': trending_models,\n",
    "                                              'popular_listings': sorted_popular_listings,\n",
    "                                              'popular_listing_scores_dict': popular_listing_scores_dictionary}\n",
    "\n",
    "    return all_city_popular_listings\n",
    "\n",
    "    popular_listings = get_popular_recommendations()\n",
    "    r.set('popular_listings', pickle.dumps(popular_listings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_active_listings():\n",
    "    all_city_active_listings = {}\n",
    "\n",
    "    for city_id in np.unique(active_listings['city_id'].values):\n",
    "        city_active_listings = active_listings.query(\"city_id == %s\" % city_id)\n",
    "        city_active_inventory_listings = city_active_listings[city_active_listings[\"is_inventory\"] == 1][\n",
    "            'listing_id'].values\n",
    "        city_active_marketplace_listings = city_active_listings[city_active_listings[\"is_inventory\"] == 0][\n",
    "            'listing_id'].values\n",
    "        all_city_active_listings[city_id] = {'inventory': city_active_inventory_listings, 'marketplace':\n",
    "                                             city_active_marketplace_listings}\n",
    "    return all_city_active_listings\n",
    "\n",
    "    active_listings = get_city_active_listings()\n",
    "    r.set('active_listings', pickle.dumps(active_listings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
